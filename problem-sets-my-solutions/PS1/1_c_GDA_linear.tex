\documentclass{article}

\usepackage{amsmath,amsfonts}
\usepackage{mathtools, amssymb}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{array}

\begin{document}


The task is to prove that classical GDA has a linear decision boundary, given that target distribution follows Bernoulli distribution $ y \sim \mathcal{B}(\phi)$ and each of two classes follows the Gaussian distribution with {\bf same} covariance matrix $p(x | y = 0) \sim \mathcal{N}(\mu_0,\,\Sigma)$ and $p(x | y = 1) \sim \mathcal{N}(\mu_1,\,\Sigma)$

\begin{align}
	p(y) \enskip &= \enskip 
		\begin{cases}
			\phi & \text{if} \enskip y = 1 \\
			1 - \phi & \text{if} \enskip y = 0
		\end{cases} \\
	p(x | y = 0) \enskip &= \enskip
		\frac 1{(2\pi)^{n/2} |\Sigma|^{n/2}} \ \text{exp}
				\left[
					-\frac12 (x - \mu_0)^T \Sigma^{-1} (x - \mu_0)
				\right] \\
	p(x | y = 1) \enskip &= \enskip
		\frac 1{(2\pi)^{n/2} |\Sigma|^{n/2}} \ \text{exp}
				\left[
					-\frac12 (x - \mu_1)^T \Sigma^{-1} (x - \mu_1)
				\right]
\end{align}
\bigbreak

Technically we need to derive the posterior distribution $p(y = 1 | x)$ and proove that it is in the form analogous to logistic regression, but with different coefficients $\theta_1 \in {\Bbb R}^n, \ \theta_0 \in {\Bbb R}$, which are functions of $\phi,\ \mu_0,\ \mu_1,\ \Sigma $.

\[
	p(y = 1| x) \ = \ \frac1{1 + \text{exp}[-(\theta_1^T x + \theta_0)]}
\]
\bigbreak

First, let's denote multidimensional Gaussian distributions in a simplified manner
\[
	p(x | y = 1 ) \ =  \ \frac1{c} \text{exp} \big[ f(x, \mu_1) \big] \qquad \text{where} \ f(x, \mu_1) = -\frac12 (x - \mu_1)^T \Sigma^{-1} (x - \mu_1)
\]
\bigbreak

We will use the Bayes rule and law of total probability for $p(x)$
\[
p(y = 1| x) = 
\frac{p(x| y=1)p(y=1)} {p(x)} = 
\frac{p(x| y=1)p(y=1)} {p(x| y=1)p(y=1) \ + \ p(x| y=0)p(y=0)} =
\]
\[
=\frac1 {1 + \frac{p(x| y=0)p(y=0)}{p(x| y=1)p(y=1)}}
=\frac1 {1 + \frac{\text{exp} (f(x, \mu_0))}{\text{exp}(f(x, \mu_1))}\frac{1-\phi}{\phi}} 
=\frac1 {1 + \text{exp} \Big [-\Big (f(x, \mu_1) - f(x, \mu_0) + log(\frac{\phi}{1-\phi}) \Big) \Big ]}
\]
\bigbreak

So we would need to simplify $f(x, \mu_1) - f(x, \mu_0)$ part. First lets consider that covariance matrix $\Sigma$ and it's inverse $\Sigma^{-1}$ are symmetrical and lets open the brackets for the quadratic form and take a closer look at two inner terms

\[
(x - \mu)^T \Sigma^{-1} (x - \mu) = x^T \Sigma^{-1} x 
- \underbrace{\mu^T \Sigma^{-1} x - x^T \Sigma^{-1} \mu }
+ \mu^T \Sigma^{-1} \mu
\]

As every symmetric matrix $S$ by spectral decomposition theorem can be represented as product of three matrices, where $Q$ - orthonormal ($Q^T Q = I$), and $\Lambda$ - diagonal, and therefore always exists a square root of symmetric matrix $S$ such that $\sqrt S \sqrt S = S$

$$ S = Q \Lambda Q^{-1} = Q \Lambda Q^T $$
$$ \sqrt S \sqrt S = Q \sqrt\Lambda Q^T \cdot Q \sqrt\Lambda Q^T  = Q \sqrt\Lambda  \sqrt\Lambda Q^T = Q \Lambda Q^T = S$$

Now lets consider two terms $\mu^T S x$ and $x^T S \mu$, taking into account that $a^T x = x^T a$ and that $\sqrt S$ is also symmetric

$$\mu^T S x = \mu^T \sqrt S \sqrt S x = (\sqrt S \mu)^T (\sqrt S x) =  (\sqrt S x)^T (\sqrt S \mu) = x^T \sqrt S \sqrt S \mu =  x^T S \mu$$

Given that result we can simplify the quadratic form 

$$(x - \mu)^T \Sigma^{-1} (x - \mu) 
= x^T \Sigma^{-1} x 
- 2 x^T \Sigma^{-1} \mu 
+ \mu^T \Sigma^{-1} \mu$$

\bigbreak

So the second order terms would cancel out in case of identical covariance matrix for each class. 

\begin{align}
f(x, \mu_1) - f(x, \mu_0) 
= -\frac12 \big [ \ & \cancel{x^T \Sigma^{-1} x} - 2 x^T\Sigma^{-1} \mu_1 + \mu_1^T \Sigma^{-1} \mu_1 \\ 
	- & \cancel{x^T \Sigma^{-1} x} + 2 x^T\Sigma^{-1} \mu_0 - \mu_0^T \Sigma^{-1} \mu_0
\ \big]=
\end{align}

$$
= x^T \Sigma^{-1} (\mu_1 - \mu_0) -
	 \frac 12 (\mu_1-\mu_0)^T \Sigma^{-1} (\mu_1 + \mu_0) =
$$

$$
= x^T \Sigma^{-1} \Delta\mu -
	\underbrace{\frac 12 \Delta\mu^T \Sigma^{-1} (\mu_1 + \mu_0)}_\text{const}
, \quad \Delta\mu = \mu_1 - \mu_0
$$

And now we can derive the complete equation for posterior distribution $p(y=1|x)$

\begin{align}
p(y = 1| x) 
&=\frac1 {1 + \text{exp} \Big [-\Big (f(x, \mu_1) - f(x, \mu_0) + log(\frac{\phi}{1-\phi}) \Big) \Big ]} \\
&=\frac1 {1 + \text{exp} \Big[ -\Big(
	x^T \underbrace{
		\Sigma^{-1} \Delta\mu
	}_{\theta_1} - 
	\underbrace{
		\frac 12 \Delta\mu^T \Sigma^{-1} (\mu_1 + \mu_0)
		+ log(\frac{\phi}{1 - \phi}
	}_{\theta_0}
	) \Big) \Big]}
\end{align}

Right now we can also show that if we assume that two classes have different covariance matrices the decision boundary would be second order hypersurface.

\[
p(y = 1| x) 
=\frac1 {1 + \text{exp}(-(
		x^T \theta_2  x +
		x^T \theta_1 + 
		\theta_0
	))}, \quad \theta_2  = \Sigma_1 - \Sigma_0 \in{\Bbb R}^{n \times n}
\]


\end{document}